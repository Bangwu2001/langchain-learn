{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc73a093",
   "metadata": {},
   "source": [
    "# langchain基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de589c",
   "metadata": {},
   "source": [
    "## ChatModels & llms\n",
    "### ChatModels\n",
    "ChatModels支持langchain Messages和OpenAI Messages\n",
    "- 支持工具调用\n",
    "- 支持结构化输出\n",
    "- 支持多模态输出\n",
    "\n",
    "### llms\n",
    "只支持字符串格式的输入和输出\n",
    "<br/>\n",
    "用的比较多的是ChatModel这类形式\n",
    "\n",
    "<hr/>\n",
    "\n",
    "支持的llm模型列表，可以通过官方网站进行查看，目前主要分为两个部分：\n",
    "- 官方合作包\n",
    "  - 一些常用的模型，官方来进行维护 \n",
    "  - [链接](https://python.langchain.com/docs/integrations/providers/)\n",
    "<br/><img src=\"./asserts/01_model_integration.png\" width=80%/>\n",
    "- 社区合作包\n",
    "  - 社区自己贡献的，官方不进行维护 \n",
    "  - [链接](https://python.langchain.com/docs/integrations/chat/)\n",
    "<br/><img src=\"./asserts/02_model_community.png\" width=80%/>\n",
    "\n",
    "从列表中，点击对应模型，可以看到各种使用场景的代码示例\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5969de48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat resp: content='Hello' additional_kwargs={} response_metadata={} id='run--d144b87f-b110-4ffb-98a3-7998b8163398'\n",
      "chat resp: content='!' additional_kwargs={} response_metadata={} id='run--d144b87f-b110-4ffb-98a3-7998b8163398'\n",
      "chat resp: content=' How' additional_kwargs={} response_metadata={} id='run--d144b87f-b110-4ffb-98a3-7998b8163398'\n",
      "chat resp: content=' can I assist you' additional_kwargs={} response_metadata={} id='run--d144b87f-b110-4ffb-98a3-7998b8163398'\n",
      "chat resp: content=' today?' additional_kwargs={} response_metadata={} id='run--d144b87f-b110-4ffb-98a3-7998b8163398'\n",
      "chat resp: content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'request_id': '458f4b05-f6d0-9208-abc7-452574f8c47f', 'token_usage': {'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'prompt_tokens_details': {'cached_tokens': 0}}} id='run--d144b87f-b110-4ffb-98a3-7998b8163398'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chatLLM = ChatTongyi(\n",
    "    streaming=True,\n",
    ")\n",
    "res = chatLLM.stream([HumanMessage(content=\"hi\")], streaming=True)\n",
    "for r in res:\n",
    "    print(\"chat resp:\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6c8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 258, 'prompt_tokens': 5, 'total_tokens': 263, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-r1', 'system_fingerprint': None, 'id': 'chatcmpl-46d32e6b-a5e4-9e04-9708-1113552beb59', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--43c08316-2711-4371-ad27-de7d02c3a3a1-0', usage_metadata={'input_tokens': 5, 'output_tokens': 258, 'total_tokens': 263, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import os\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-r1\", #指定模型名称\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), #api-key\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", #请求地址\n",
    ")\n",
    "message = \"介绍一下你自己\"\n",
    "llm.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60689019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='您好！我是通义千问，阿里巴巴集团旗下的超大规模语言模型。我能够回答问题、创作文字，例如写故事、写公文、写邮件、写剧本、逻辑推理、编程等等，还能表达观点，玩游戏等。我熟练掌握多种语言，包括但不限于中文、英文、德语、法语、西班牙语等。\\n\\n我的训练数据完全来自于阿里巴巴集团内部的历史积累，这使得我在多个领域和任务上表现出色。如果您有任何问题或需要帮助，请随时告诉我！我会尽力提供支持和解答。希望我能成为您学习和工作的得力助手！', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 123, 'prompt_tokens': 10, 'total_tokens': 133, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-cea3f7fc-799e-9e98-8d99-3da0738d69c8', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--f833deb5-58ce-47cc-a324-1c0c3e337676-0', usage_metadata={'input_tokens': 10, 'output_tokens': 123, 'total_tokens': 133, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import os\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-plus\", #指定模型名称\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), #api-key\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", #请求地址\n",
    ")\n",
    "message = \"介绍一下你自己\"\n",
    "llm.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d6f66",
   "metadata": {},
   "source": [
    "## langchain封装模型标准\n",
    "langchain使用标准参数、事件、输入和输出来驱动大模型，不过这些仅在官方合作包中做要求，在社区合作包中不做要求"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7d07d",
   "metadata": {},
   "source": [
    "### 标准参数\n",
    "- Model:模型名称\n",
    "- Temprature:生成随机度，越大模型输出创造性越强\n",
    "- Timeout:超时时间\n",
    "- Max_tokens:输出长度\n",
    "- Stop:指定停止字符\n",
    "- Max_retries:最大重试数\n",
    "- Api_key:API Key\n",
    "- Base_url:模型代理地址\n",
    "- Rater_limiter:请求速率限制\n",
    "\n",
    "一般用的最多的参数是:\n",
    "- Model\n",
    "- Temprature\n",
    "- Api_Key\n",
    "- Base_url\n",
    "\n",
    "<br/>**注意**<br/>\n",
    "这些标准参数只对提供的API开放了同样参数的模型有用，比如有的模型提供的api中没有指定Rater_limiter，则在langchain中也无效，langchain只是在各自大模型api基础上进行了统一规范的封装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c785a3",
   "metadata": {},
   "source": [
    "### 事件\n",
    "#### 标准事件\n",
    "- invoke:模型主要调用方法\n",
    "- Stream:流式输出方法\n",
    "- Batch:批量模型请求方法\n",
    "- Bind_tools:在模型执行的时候绑定工具\n",
    "- With_structured_output:基于invoke的结构化输出\n",
    "\n",
    "#### 其他事件\n",
    "- ainvoke:异步调用模型方法\n",
    "- astream:异步流式输出\n",
    "- abatch:异步批量处理\n",
    "- astream_events:异步流事件,可以在模型调用不同阶段通过响应不同事件最初对应操作\n",
    "- With_retry:调用失败时重试\n",
    "- With_fallback:失败降级处理事件\n",
    "- Configurable_fields:模型运行时参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化模型\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-plus\", #指定模型名称\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), #api-key\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", #请求地址\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b27309f",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': 'invalid_parameter_error', 'param': None, 'message': \"<400> InternalError.Algo.InvalidParameter: 'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error'}, 'id': 'chatcmpl-98f35080-9d5e-9ae8-bb49-6f4121fc4a07', 'request_id': '98f35080-9d5e-9ae8-bb49-6f4121fc4a07'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m     rating: Optional[\u001b[38;5;28mint\u001b[39m] = Field(\n\u001b[32m     17\u001b[39m         default=\u001b[38;5;28;01mNone\u001b[39;00m, description=\u001b[33m\"\u001b[39m\u001b[33mHow funny the joke is, from 1 to 10\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m     )\n\u001b[32m     19\u001b[39m structured_llm = llm.with_structured_output(Joke)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mstructured_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m给我讲一个关于程序员的笑话\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramTools\\anaconda3\\envs\\agent\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramTools\\anaconda3\\envs\\agent\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5431\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5424\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5425\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5426\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5429\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5430\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5432\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5433\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5434\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5435\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramTools\\anaconda3\\envs\\agent\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramTools\\anaconda3\\envs\\agent\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramTools\\anaconda3\\envs\\agent\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramTools\\anaconda3\\envs\\agent\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramTools\\anaconda3\\envs\\agent\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:973\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    971\u001b[39m         response = \u001b[38;5;28mself\u001b[39m.root_client.beta.chat.completions.parse(**payload)\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m         \u001b[43m_handle_openai_bad_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_responses_api(payload):\n\u001b[32m    975\u001b[39m     original_schema_obj = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramTools\\anaconda3\\envs\\agent\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:971\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    969\u001b[39m payload.pop(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    973\u001b[39m     _handle_openai_bad_request(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramTools\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py:158\u001b[39m, in \u001b[36mCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    153\u001b[39m         response_format=response_format,\n\u001b[32m    154\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    155\u001b[39m         input_tools=tools,\n\u001b[32m    156\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramTools\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramTools\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'code': 'invalid_parameter_error', 'param': None, 'message': \"<400> InternalError.Algo.InvalidParameter: 'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error'}, 'id': 'chatcmpl-98f35080-9d5e-9ae8-bb49-6f4121fc4a07', 'request_id': '98f35080-9d5e-9ae8-bb49-6f4121fc4a07'}"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen-plus\", #指定模型名称\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), #api-key\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", #请求地址\n",
    ")\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(\n",
    "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
    "    )\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "structured_llm.invoke(\"给我讲一个关于程序员的笑话\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da628c",
   "metadata": {},
   "source": [
    "#### invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d65c9e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='您好！我是通义千问，阿里巴巴集团旗下的超大规模语言模型。我能够回答问题、创作文字，比如写故事、公文、邮件、剧本等，还能进行逻辑推理、编程，甚至表达观点和玩游戏。我在多国语言上都有很好的掌握，能为您提供多样化的帮助。如果您有任何问题或需要创作的内容，欢迎随时告诉我！', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 10, 'total_tokens': 86, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-fc53fcf2-947c-9c5d-bb10-8bbc6ec45f2d', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--9c9fdbf9-7737-4132-baf4-129466da934d-0', usage_metadata={'input_tokens': 10, 'output_tokens': 76, 'total_tokens': 86, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"介绍一下你自己\"\n",
    "llm.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684a528",
   "metadata": {},
   "source": [
    "#### stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca71a300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "您好\n",
      "！\n",
      "我是\n",
      "通义千问\n",
      "，阿里巴巴集团旗下的\n",
      "超大规模语言模型\n",
      "。我的设计目标\n",
      "是成为一款能够\n",
      "理解并生成自然\n",
      "语言、代码等多种\n",
      "类型内容的多\n",
      "模态预训练\n",
      "模型。无论是撰写\n",
      "文章、创作故事\n",
      "、编写公文\n",
      "、写邮件、\n",
      "写剧本，还是\n",
      "进行逻辑推理、\n",
      "编程等任务，\n",
      "我都能提供帮助\n",
      "。同时，我也\n",
      "擅长表达观点和\n",
      "玩游戏，旨在为\n",
      "用户提供丰富而有趣的\n",
      "交互体验。如果您\n",
      "有任何问题或需要\n",
      "帮助，请随时告诉我\n",
      "！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(message):\n",
    "  print(chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609290c",
   "metadata": {},
   "source": [
    "#### batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87dddfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='您好！我是通义千问，阿里巴巴集团旗下的通义实验室自主研发的超大规模语言模型。我能够回答问题、创作文字，比如写故事、写公文、写邮件、写剧本、逻辑推理、编程等等，还能表达观点，玩游戏等。\\n\\n我支持多种语言，包括但不限于中文、英文、德语、法语、西班牙语等，可以满足国际化的使用需求。我的训练数据完全来自于阿里巴巴集团内部的历史积累，这使得我在处理复杂任务和理解深层次的问题上具有独特的优势。\\n\\n如果您有任何问题或需要帮助，请随时告诉我！我会尽力提供最准确和有用的信息。希望我能成为您生活和工作中得力的助手！', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 141, 'prompt_tokens': 10, 'total_tokens': 151, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-a62f6427-b8bb-9302-8564-b6c60ca37dfd', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--1b05284c-3c15-4dfc-b7cb-cf4a02f674d8-0', usage_metadata={'input_tokens': 10, 'output_tokens': 141, 'total_tokens': 151, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}),\n",
       " AIMessage(content='我有以下几个主要优势：\\n\\n1. **超大规模参数量**：这使我能够更好地理解和生成复杂的语言结构。\\n2. **大量训练数据**：基于大量的互联网文本进行训练，使我能够理解各种主题和上下文。\\n3. **对话理解能力**：经过多轮迭代和优化，我的对话理解能力得到了显著提高，能够准确理解与用户交互的对话历史，为用户提供更自然、流畅的对话体验。\\n4. **代码写作能力**：经过充分的训练，我具有多种编程语言的理解和生成能力。\\n5. **多语言支持**：除了中文，我还支持其他多种语言，满足国际化的使用需求。\\n\\n这些优势使我在多个应用场景中表现出色，无论是撰写文章、创作故事、编写代码，还是进行多语言翻译和对话交流等。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 168, 'prompt_tokens': 11, 'total_tokens': 179, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-e564045a-722c-9e56-a442-9506b117ddeb', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--c81786e6-682b-47df-8b69-bc41432fe840-0', usage_metadata={'input_tokens': 11, 'output_tokens': 168, 'total_tokens': 179, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.batch([\"介绍一下你自己\",\"你有哪些优势\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c35a7c",
   "metadata": {},
   "source": [
    "#### astream_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16611d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event=on_chat_model_start|name=ChatOpenAI|data={'input': '介绍一下你自己'}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='您好', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='！', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='我是', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='通义千问', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='，阿里巴巴集团旗下的', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='通义实验室自主研发', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='的超大规模语言', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='模型。我能够', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='回答问题、创作', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='文字，比如写', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='故事、写公', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='文、写邮件', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='、写剧本、', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='逻辑推理、编程', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='等等，还能表达', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='观点，玩游戏等', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='。我熟练掌握', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='多种语言，包括', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='但不限于中文、英文', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='、德语、', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='法语、西班牙', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='语等。\\n\\n我的', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='训练数据完全来自于', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='阿里巴巴集团内部的历史', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='积累，这使得', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='我在处理复杂任务', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='和理解深层次信息', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='方面具有独特的优势', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='。无论是日常生活中的', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='小问题还是专业', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='领域的复杂难题，', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='我都可以尽力为您提供', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='帮助和支持。\\n\\n如果您', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='有任何问题或需要', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='帮助，请随时告诉我', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='！我很乐意为您', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='服务。', additional_kwargs={}, response_metadata={}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_stream|name=ChatOpenAI|data={'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'qwen-plus'}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n",
      "event=on_chat_model_end|name=ChatOpenAI|data={'output': AIMessageChunk(content='您好！我是通义千问，阿里巴巴集团旗下的通义实验室自主研发的超大规模语言模型。我能够回答问题、创作文字，比如写故事、写公文、写邮件、写剧本、逻辑推理、编程等等，还能表达观点，玩游戏等。我熟练掌握多种语言，包括但不限于中文、英文、德语、法语、西班牙语等。\\n\\n我的训练数据完全来自于阿里巴巴集团内部的历史积累，这使得我在处理复杂任务和理解深层次信息方面具有独特的优势。无论是日常生活中的小问题还是专业领域的复杂难题，我都可以尽力为您提供帮助和支持。\\n\\n如果您有任何问题或需要帮助，请随时告诉我！我很乐意为您服务。', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'qwen-plus'}, id='run--84a54327-1289-400b-860f-e3137e8bcfdc')}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async for event in llm.astream_events(\"介绍一下你自己\",version=\"v2\"):\n",
    "  print(f\"event={event['event']}|name={event['name']}|data={event['data']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3f247",
   "metadata": {},
   "source": [
    "#### with_structured_output\n",
    "[官方文档](https://python.langchain.com/docs/concepts/structured_outputs/#structured-output-method)\n",
    "\n",
    "- 注意：很多模型可能无法很好的适配with_structured_output，可以通过提示词约束来实现格式化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65c24029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='回答问题清晰明了，并遵循指定格式：\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"summary\": {\"description\": \"答案的简短总结\", \"title\": \"Summary\", \"type\": \"string\"}, \"details\": {\"description\": \"详细的要点列表\", \"items\": {\"type\": \"string\"}, \"title\": \"Details\", \"type\": \"array\"}, \"references\": {\"anyOf\": [{\"items\": {\"type\": \"string\"}, \"type\": \"array\"}, {\"type\": \"null\"}], \"description\": \"参考资料列表\", \"title\": \"References\"}}, \"required\": [\"summary\", \"details\", \"references\"]}\\n```\\n\\n问题：如何提高编程技能？'\n",
      "summary='提高编程技能需要系统学习、持续实践和不断反思。' details=['学习基础知识：掌握语言语法、算法、数据结构和设计模式。', '动手实践：通过项目实战、开源贡献或编程挑战巩固知识。', '阅读优秀代码：分析开源项目或技术书籍中的代码范例。', '接受反馈：参与代码评审，吸收他人建议以改进代码质量。', '定期复盘：总结项目经验，记录技术笔记并优化学习路径。', '保持好奇心：探索新技术，关注行业动态和技术博客。'] references=['《代码大全》Steve McConnell', '《程序员修炼之道》Andrew Hunt', 'LeetCode/CodeWars等编程练习平台']\n",
      "{'summary': '提高编程技能需要系统学习、持续实践和不断反思。', 'details': ['学习基础知识：掌握语言语法、算法、数据结构和设计模式。', '动手实践：通过项目实战、开源贡献或编程挑战巩固知识。', '阅读优秀代码：分析开源项目或技术书籍中的代码范例。', '接受反馈：参与代码评审，吸收他人建议以改进代码质量。', '定期复盘：总结项目经验，记录技术笔记并优化学习路径。', '保持好奇心：探索新技术，关注行业动态和技术博客。'], 'references': ['《代码大全》Steve McConnell', '《程序员修炼之道》Andrew Hunt', 'LeetCode/CodeWars等编程练习平台']}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 定义Pydantic模型\n",
    "class Answer(BaseModel):\n",
    "    summary: str = Field(description=\"答案的简短总结\")\n",
    "    details: List[str] = Field(description=\"详细的要点列表\")\n",
    "    references: Optional[List[str]] = Field(description=\"参考资料列表\")\n",
    "\n",
    "# 创建输出解析器\n",
    "parser = PydanticOutputParser(pydantic_object=Answer)\n",
    "\n",
    "# 创建提示模板\n",
    "prompt = PromptTemplate(\n",
    "    template=\"回答问题清晰明了，并遵循指定格式：\\n{format_instructions}\\n\\n问题：{query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# 初始化模型并运行\n",
    "model = llm\n",
    "_input = prompt.format_prompt(query=\"如何提高编程技能？\")\n",
    "print(_input)\n",
    "output = model.invoke(_input.to_string())\n",
    "\n",
    "# 解析输出\n",
    "parsed_output = parser.parse(output.content)\n",
    "print(parsed_output)\n",
    "print(dict(parsed_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1aec2",
   "metadata": {},
   "source": [
    "### 输入输出\n",
    "#### OpenAI Messages\n",
    "- SystemMessage:用于传导对话内容\n",
    "- HumanMessage:用户输入的内容\n",
    "- AIMessage:模型响应的内容\n",
    "- 多模态\n",
    "\n",
    "#### LangchainMessage\n",
    "- SystemMessage:系统角色\n",
    "- HumanMessage:用户角色\n",
    "- AIMessage:应用助理角色\n",
    "- AIMessageChunk:应用助理流式输出\n",
    "- ToolMessage:工具角色\n",
    "- RemoveMessage:LangGraph聊天记录\n",
    "\n",
    "##### AIMessage（模型响应内容一般包含的属性）\n",
    "- content:通常为字符串，也可以是内容块列表，模型生成的回复内容\n",
    "- Tool_calls:与消息相关的工具调用\n",
    "- Invalid_tool_calls:工具调用与消息相关解析错误\n",
    "- Usage_metadata:元数据（输入输出token数，总计token数）\n",
    "- id:消息唯一标识符\n",
    "- Response_metadata:响应元数据（响应头、token计数等）\n",
    "\n",
    "注意：不同大模型提供的内容属性可能并不相同，且行业暂无统一标注好呢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500697d4",
   "metadata": {},
   "source": [
    "## 模型相关知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b2558e",
   "metadata": {},
   "source": [
    "### 上下文窗口"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6ce7d",
   "metadata": {},
   "source": [
    "### token的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ba396",
   "metadata": {},
   "source": [
    "### 模型速率限制与缓存机制\n",
    "#### 速率限制\n",
    "- TPM:每分钟Token数\n",
    "- RPM:每分钟请求数\n",
    "\n",
    "https://python.langchain.com/docs/concepts/chat_models/#rate-limiting\n",
    "#### 缓存机制\n",
    "langchain内部内置了缓存机制，当请求内容相似时，可以直接走缓存，减少对模型的调用\n",
    "\n",
    "https://python.langchain.com/docs/concepts/chat_models/#caching\n",
    "\n",
    "#### langchain与ollama\n",
    "可以通过ollama本地部署模型，然后通过langchain来进行调用\n",
    "\n",
    "https://python.langchain.com/docs/integrations/chat/ollama/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910f9d0",
   "metadata": {},
   "source": [
    "## 工具调用\n",
    "### 大模型工具调用的原理\n",
    "1. 创建工具（比如定义一个函数）\n",
    "2. 将工具绑定到模型\n",
    "3. 模型判断针对输入的问题是否需要调用工具\n",
    "4. 模型会生成符合工具调用的结构化数据\n",
    "5. 使用工具\n",
    "6. 返回工具使用得到的结果\n",
    "7. 将结果返回给模型用于增强回答\n",
    "\n",
    "**注意**\n",
    "- 模型只是生成工具调用所需的参数，并不直接运行工具\n",
    "- 并非所有模型都支持工具调用\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
